{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24416112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad6fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a9eb1",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Configure which GPU(s) to use\n",
    "    tf.config.experimental.set_visible_devices(gpus, 'GPU')\n",
    "    # Enable memory growth for each GPU\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b068f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46491e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ad448",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/train/\"\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "for classes in tqdm(os.listdir(path)):\n",
    "    class_ = path + classes\n",
    "    for images in os.listdir(class_):\n",
    "        try:\n",
    "            image_path = class_ + '/' + images\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image,(224,224))\n",
    "            image = np.expand_dims(image,axis=0)\n",
    "            features = model.predict(image)[0]\n",
    "            trainX.append(features)\n",
    "            trainY.append(class_.split('/')[-1])\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.array(trainX)\n",
    "trainY = np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/test/\"\n",
    "\n",
    "testX = []\n",
    "testY = []\n",
    "for classes in tqdm(os.listdir(path)):\n",
    "    class_ = path + classes\n",
    "    for images in os.listdir(class_):\n",
    "        try:\n",
    "            image_path = class_ + '/' + images\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image,(224,224))\n",
    "            image = np.expand_dims(image,axis=0)\n",
    "            features = model.predict(image)[0]\n",
    "            testX.append(features)\n",
    "            testY.append(class_.split('/')[-1])\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c664fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.array(testX)\n",
    "testY = np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "trainY = le.fit_transform(trainY)\n",
    "testY = le.fit_transform(testY)\n",
    "trainY = to_categorical(trainY, 6)\n",
    "testY = to_categorical(testY, 6)\n",
    "print(trainX.shape,testX.shape,trainY.shape,testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(trainX, open('trainX.pkl', 'wb'))\n",
    "pickle.dump(trainY, open('trainY.pkl', 'wb'))\n",
    "pickle.dump(testX, open('testX.pkl', 'wb'))\n",
    "pickle.dump(testY, open('testY.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38cf4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61486560",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX  = pickle.load(open('trainX.pkl', 'rb'))\n",
    "trainY = pickle.load(open('trainY.pkl', 'rb'))\n",
    "testX = pickle.load(open('testX.pkl', 'rb'))\n",
    "testY = pickle.load(open('testY.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([trainX, testX])\n",
    "y = np.concatenate([trainY, testY])\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(random_state=42, max_iter=256,hidden_layer_sizes=1024,shuffle=True,\n",
    "                      activation='logistic', solver='adam',learning_rate='adaptive',validation_fraction=0.15,early_stopping=True,verbose=True)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85969620",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('mlp_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55750586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e731e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['explicit_moderation', 'gore_moderation', 'intoxicants_moderation', 'sfw_moderation', 'suggestive_moderation', 'weapons_moderation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator LabelBinarizer from version 1.1.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator MLPClassifier from version 1.1.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "mymodel = pickle.load(open('mlp_model.pkl', 'rb'))\n",
    "classes = open(\"classes.txt\",'r').read().split('\\n')[:-1]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deda5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/validation/\"\n",
    "\n",
    "validY = []\n",
    "predY = []\n",
    "for class_path in tqdm(os.listdir(path)):\n",
    "    class_ = path + class_path\n",
    "    for images in os.listdir(class_):\n",
    "        try:\n",
    "            image_path = class_ + '/' + images\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image,(224,224))\n",
    "            image = np.expand_dims(image,axis=0)\n",
    "            features = model.predict(image)\n",
    "            #features = np.expand_dims(features,axis=0)\n",
    "            pred = mymodel.predict(features)[0]\n",
    "            name = classes[np.argmax(pred)]\n",
    "            predY.append(name)\n",
    "            validY.append(class_.split('/')[2])\n",
    "        except:\n",
    "            print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad744c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validY), len(predY), validY[0], predY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "print(classification_report(validY, predY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(validY, predY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08210550",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(conf_matrix,\n",
    "            annot=True,\n",
    "            fmt='g',\n",
    "            xticklabels=classes,\n",
    "            yticklabels=classes,\n",
    "    )\n",
    "plt.ylabel('Prediction',fontsize=13)\n",
    "plt.xlabel('Actual',fontsize=13)\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ef275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(validY, predY)\n",
    "precision = precision_score(validY, predY,average='weighted')\n",
    "recall = recall_score(validY, predY,average='weighted')\n",
    "f1 = f1_score(validY, predY,average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d06fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca52a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfw_moderation\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"person.jpg\")\n",
    "image = cv2.resize(image,(224,224))\n",
    "image = np.expand_dims(image,axis=0)\n",
    "features = model.predict(image)\n",
    "pred = mymodel.predict(features)[0]\n",
    "name = classes[np.argmax(pred)]\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256ea10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
